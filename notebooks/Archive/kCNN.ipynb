{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T09:32:27.206011600Z",
     "start_time": "2025-12-24T09:18:03.296684Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from flamekit.io_fields import field_path\n",
    "from flamekit.io_fronts import Case\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as mtri\n",
    "\n",
    "# Optional MPI (safe in serial)\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.rank\n",
    "except Exception:\n",
    "    comm = None\n",
    "    rank = 0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "TIME_STEP_START = 200\n",
    "TIME_STEP_END   = 269\n",
    "\n",
    "PHI      = 0.40\n",
    "LAT_SIZE = \"025\"\n",
    "POST     = True\n",
    "\n",
    "BASE_DIR  = Path(\"../data/isocontours\")\n",
    "VAR_NAME  = \"T\"                 # field to model\n",
    "SORT_COLS = [\"x\", \"y\"]\n",
    "COORD_TOL = 0.0                 # 0 -> exact match; >0 -> np.allclose atol=COORD_TOL\n",
    "\n",
    "# Keep only points with x > X_THESHOLD (applied consistently across all timesteps)\n",
    "X_THESHOLD = 300\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Koopman-CNN settings\n",
    "# ============================================================\n",
    "\n",
    "DEVICE = \"cuda\"   # \"cuda\" or \"cpu\"\n",
    "DT = 1.0          # index-time step (kept for completeness)\n",
    "\n",
    "Z_DIM = 16        # latent dim (8..64 typical)\n",
    "EPOCHS = 600\n",
    "BATCHES_PER_EPOCH = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "ROLLOUT_LEN = 4   # multi-step training horizon (<= n_snaps-1)\n",
    "\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 1e-7\n",
    "\n",
    "# Loss weights\n",
    "W_RECON = 1.0            # reconstruction of each snapshot\n",
    "W_PRED  = 1.0            # multi-step prediction in observation space (decoded)\n",
    "W_LAT   = 0.1            # latent-space multi-step consistency\n",
    "W_STAB  = 1e-3           # spectral radius penalty\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# File helpers\n",
    "# ============================================================\n",
    "\n",
    "def field_csv_path(base_dir: Path, phi: float, lat_size: str, time_step: int, post: bool) -> Path:\n",
    "    case = Case(\n",
    "        base_dir=base_dir,\n",
    "        phi=phi,\n",
    "        lat_size=lat_size,\n",
    "        time_step=time_step,\n",
    "        post=post,\n",
    "    )\n",
    "    return field_path(case)\n",
    "\n",
    "def read_field_sorted(path: Path, var_name: str, sort_cols: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file:\\n  {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    missing = [c for c in (sort_cols + [var_name]) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path.name}: missing columns {missing}\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=sort_cols + [var_name])\n",
    "    df = df.sort_values(sort_cols, kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    coords = df[sort_cols].to_numpy(dtype=np.float64)\n",
    "    values = df[var_name].to_numpy(dtype=np.float64)\n",
    "    return coords, values\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build snapshot matrix X (n_points, n_snaps) without interpolation\n",
    "# ============================================================\n",
    "\n",
    "times = list(range(TIME_STEP_START, TIME_STEP_END + 1))\n",
    "\n",
    "ref_path = field_csv_path(BASE_DIR, PHI, LAT_SIZE, times[0], POST)\n",
    "coords_ref_full, snap0_full = read_field_sorted(ref_path, VAR_NAME, SORT_COLS)\n",
    "\n",
    "# Apply x-threshold mask ONCE (from reference coords), then apply to all timesteps\n",
    "x_ref = coords_ref_full[:, 0]\n",
    "mask_x = x_ref > X_THESHOLD\n",
    "\n",
    "coords_ref = coords_ref_full[mask_x]\n",
    "snap0 = snap0_full[mask_x]\n",
    "\n",
    "n_points = coords_ref.shape[0]\n",
    "n_snaps = len(times)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"Reference timestep: {times[0]}\")\n",
    "    print(f\"X_THESHOLD={X_THESHOLD} -> keeping {n_points}/{coords_ref_full.shape[0]} points\")\n",
    "    print(f\"n_points={n_points}, n_snapshots={n_snaps}\")\n",
    "    print(f\"Reading: {ref_path}\")\n",
    "\n",
    "snapshots = [snap0]\n",
    "for t in times[1:]:\n",
    "    path = field_csv_path(BASE_DIR, PHI, LAT_SIZE, t, POST)\n",
    "    coords_t_full, snap_t_full = read_field_sorted(path, VAR_NAME, SORT_COLS)\n",
    "\n",
    "    # Coordinate match check on FULL coords\n",
    "    if coords_t_full.shape[0] != coords_ref_full.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Inconsistent n_points (full) at timestep {t}: {coords_t_full.shape[0]} vs {coords_ref_full.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    if COORD_TOL == 0.0:\n",
    "        same = np.array_equal(coords_t_full, coords_ref_full)\n",
    "    else:\n",
    "        same = np.allclose(coords_t_full, coords_ref_full, atol=COORD_TOL, rtol=0.0)\n",
    "\n",
    "    if not same:\n",
    "        raise ValueError(\n",
    "            f\"Coordinates mismatch at timestep {t}.\\n\"\n",
    "            f\"Set COORD_TOL>0 (if only small floating diffs) or interpolate/regrid.\"\n",
    "        )\n",
    "\n",
    "    snap_t = snap_t_full[mask_x]\n",
    "    if snap_t.shape[0] != n_points:\n",
    "        raise RuntimeError(\"Masking produced inconsistent point count. Check X_THESHOLD and sorting.\")\n",
    "\n",
    "    snapshots.append(snap_t)\n",
    "\n",
    "# X: (n_points, n_snaps)\n",
    "X = np.stack(snapshots, axis=1).astype(np.float64)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Grid inference / gridding utilities (for CNN)\n",
    "# ============================================================\n",
    "\n",
    "def infer_structured_grid(coords_xy: np.ndarray) -> tuple[bool, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Try to infer a structured (x,y) grid from scattered coords.\n",
    "    Returns:\n",
    "      structured_ok, xs, ys, ix, iy\n",
    "    where xs are unique sorted x coords, ys unique sorted y coords,\n",
    "    and ix,iy are integer indices per point such that grid[iy, ix] maps to point value.\n",
    "    \"\"\"\n",
    "    x = coords_xy[:, 0]\n",
    "    y = coords_xy[:, 1]\n",
    "    xs = np.unique(x)\n",
    "    ys = np.unique(y)\n",
    "    nx = xs.size\n",
    "    ny = ys.size\n",
    "\n",
    "    if coords_xy.shape[0] != nx * ny:\n",
    "        return False, xs, ys, None, None\n",
    "\n",
    "    # map each point to grid indices\n",
    "    ix = np.searchsorted(xs, x)\n",
    "    iy = np.searchsorted(ys, y)\n",
    "\n",
    "    # check that every (ix,iy) is present exactly once\n",
    "    lin = iy * nx + ix\n",
    "    if np.unique(lin).size != lin.size:\n",
    "        return False, xs, ys, None, None\n",
    "\n",
    "    return True, xs, ys, ix, iy\n",
    "\n",
    "\n",
    "def points_to_grid_structured(values: np.ndarray, nx: int, ny: int, ix: np.ndarray, iy: np.ndarray) -> np.ndarray:\n",
    "    grid = np.empty((ny, nx), dtype=np.float32)\n",
    "    grid[iy, ix] = values.astype(np.float32)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def points_to_grid_triangulated(coords_xy: np.ndarray, values: np.ndarray, xs: np.ndarray, ys: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fallback: interpolate scattered values onto a regular grid defined by xs, ys using matplotlib tri interpolation.\n",
    "    \"\"\"\n",
    "    x = coords_xy[:, 0].astype(np.float64)\n",
    "    y = coords_xy[:, 1].astype(np.float64)\n",
    "    tri = mtri.Triangulation(x, y)\n",
    "    interp = mtri.LinearTriInterpolator(tri, values.astype(np.float64))\n",
    "    Xg, Yg = np.meshgrid(xs, ys)\n",
    "    Zg = interp(Xg, Yg)  # masked array\n",
    "    Z = np.asarray(Zg.filled(np.nan), dtype=np.float32)\n",
    "\n",
    "    # Fill NaNs (outside convex hull) with nearest-neighbor-ish fallback: use mean as safe default\n",
    "    if np.isnan(Z).any():\n",
    "        fill = np.nanmean(Z)\n",
    "        if not np.isfinite(fill):\n",
    "            fill = 0.0\n",
    "        Z = np.nan_to_num(Z, nan=float(fill)).astype(np.float32)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def grid_to_points_structured(grid: np.ndarray, ix: np.ndarray, iy: np.ndarray) -> np.ndarray:\n",
    "    return grid[iy, ix].astype(np.float64)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Prepare image sequence for CNN: (T, 1, ny, nx)\n",
    "# ============================================================\n",
    "\n",
    "structured_ok, xs, ys, ix, iy = infer_structured_grid(coords_ref)\n",
    "nx = xs.size\n",
    "ny = ys.size\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"Grid inference: structured_ok={structured_ok}, nx={nx}, ny={ny}, n_points={n_points}\")\n",
    "\n",
    "# Build gridded snapshots\n",
    "imgs = []\n",
    "for k in range(n_snaps):\n",
    "    v = X[:, k]\n",
    "    if structured_ok:\n",
    "        g = points_to_grid_structured(v, nx=nx, ny=ny, ix=ix, iy=iy)\n",
    "    else:\n",
    "        # regular grid definition for fallback\n",
    "        # (using inferred unique xs/ys still, but may be non-rectangular coverage)\n",
    "        g = points_to_grid_triangulated(coords_ref, v, xs, ys)\n",
    "    imgs.append(g)\n",
    "\n",
    "# imgs_np: (T, ny, nx)\n",
    "imgs_np = np.stack(imgs, axis=0).astype(np.float32)\n",
    "\n",
    "# Normalize per-pixel over time (like your per-DOF normalization, but 2D)\n",
    "mean_img = imgs_np.mean(axis=0, keepdims=True)                 # (1, ny, nx)\n",
    "std_img  = imgs_np.std(axis=0, keepdims=True) + 1e-6           # (1, ny, nx)\n",
    "imgs_n   = (imgs_np - mean_img) / std_img                      # (T, ny, nx)\n",
    "\n",
    "# Add channel dim: (T, 1, ny, nx)\n",
    "imgs_n = imgs_n[:, None, :, :]\n",
    "\n",
    "T_total = imgs_n.shape[0]\n",
    "if ROLLOUT_LEN >= T_total:\n",
    "    raise ValueError(f\"ROLLOUT_LEN={ROLLOUT_LEN} must be < number of snapshots T={T_total}.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Koopman-CNN (PyTorch)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\" and not torch.cuda.is_available():\n",
    "    DEVICE = \"cpu\"\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Using device:\", device)\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "X_torch = torch.from_numpy(imgs_n).to(device)  # (T,1,ny,nx)\n",
    "\n",
    "\n",
    "def spectral_radius(K: torch.Tensor) -> torch.Tensor:\n",
    "    eigvals = torch.linalg.eigvals(K)\n",
    "    rho = torch.max(torch.abs(eigvals))\n",
    "    return rho.real\n",
    "\n",
    "\n",
    "def sample_batch_indices(T: int, rollout_len: int, batch_size: int) -> list[int]:\n",
    "    max_start = T - (rollout_len + 1)\n",
    "    if max_start < 0:\n",
    "        raise ValueError(f\"Not enough snapshots: T={T}, rollout_len={rollout_len}\")\n",
    "    idx = np.random.randint(0, max_start + 1, size=batch_size)\n",
    "    return idx.tolist()\n",
    "\n",
    "\n",
    "def make_batch(Xseq: torch.Tensor, idx0: list[int], rollout_len: int) -> torch.Tensor:\n",
    "    # (B, L+1, 1, ny, nx)\n",
    "    seqs = [Xseq[i : i + rollout_len + 1] for i in idx0]\n",
    "    return torch.stack(seqs, dim=0)\n",
    "\n",
    "\n",
    "class KoopmanCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional autoencoder + linear latent dynamics:\n",
    "      z_{k+1} = z_k K^T\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: downsample + pool to fixed 4x4\n",
    "        self.enc_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 4, stride=2, padding=1), nn.ReLU(inplace=True),  # /2\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.ReLU(inplace=True),  # /4\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.ReLU(inplace=True), # /8\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.enc_fc = nn.Linear(128 * 4 * 4, z_dim)\n",
    "\n",
    "        # Decoder: z -> 128x4x4 then upsample to ~64x64 then interpolate to (ny,nx)\n",
    "        self.dec_fc = nn.Linear(z_dim, 128 * 4 * 4)\n",
    "        self.dec_deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(inplace=True),  # 8x8\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(inplace=True),   # 16x16\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1), nn.ReLU(inplace=True),   # 32x32\n",
    "            nn.ConvTranspose2d(16, 8, 4, stride=2, padding=1), nn.ReLU(inplace=True),    # 64x64\n",
    "            nn.Conv2d(8, 1, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        # Linear Koopman operator in latent space\n",
    "        self.K = nn.Parameter(torch.eye(z_dim))\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,ny,nx)\n",
    "        h = self.enc_conv(x)\n",
    "        h = self.pool(h)\n",
    "        h = h.flatten(1)\n",
    "        z = self.enc_fc(h)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: torch.Tensor, out_hw: tuple[int, int]) -> torch.Tensor:\n",
    "        # z: (B,z_dim)\n",
    "        B = z.shape[0]\n",
    "        h = self.dec_fc(z).view(B, 128, 4, 4)\n",
    "        y = self.dec_deconv(h)  # (B,1,~64,~64)\n",
    "        y = F.interpolate(y, size=out_hw, mode=\"bilinear\", align_corners=False)\n",
    "        return y\n",
    "\n",
    "    def step_latent(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: (B,z_dim) as row vectors\n",
    "        return z @ self.K.T\n",
    "\n",
    "\n",
    "model = KoopmanCNN(z_dim=Z_DIM).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"Training KoopmanCNN: z_dim={Z_DIM}, T={T_total}, rollout_len={ROLLOUT_LEN}, img=({ny},{nx})\")\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(BATCHES_PER_EPOCH):\n",
    "        idx0 = sample_batch_indices(T_total, ROLLOUT_LEN, batch_size=BATCH_SIZE)\n",
    "        Xb = make_batch(X_torch, idx0, ROLLOUT_LEN)  # (B,L+1,1,ny,nx)\n",
    "        B, Lp1, C, H, W = Xb.shape\n",
    "        L = Lp1 - 1\n",
    "\n",
    "        # Encode all frames in batch\n",
    "        Xflat = Xb.reshape(B * (L + 1), 1, H, W)\n",
    "        z_true = model.encode(Xflat).reshape(B, L + 1, Z_DIM)\n",
    "\n",
    "        # Reconstruction\n",
    "        X_recon = model.decode(z_true.reshape(B * (L + 1), Z_DIM), out_hw=(H, W)).reshape(B, L + 1, 1, H, W)\n",
    "        loss_recon = mse(X_recon, Xb)\n",
    "\n",
    "        # Latent rollout from z0\n",
    "        z_list = [z_true[:, 0, :]]\n",
    "        for _k in range(L):\n",
    "            z_list.append(model.step_latent(z_list[-1]))\n",
    "        z_pred = torch.stack(z_list, dim=1)  # (B,L+1,Z_DIM)\n",
    "\n",
    "        # Decode predicted sequence\n",
    "        X_pred = model.decode(z_pred.reshape(B * (L + 1), Z_DIM), out_hw=(H, W)).reshape(B, L + 1, 1, H, W)\n",
    "\n",
    "        # Prediction loss (exclude k=0)\n",
    "        loss_pred = mse(X_pred[:, 1:, ...], Xb[:, 1:, ...])\n",
    "\n",
    "        # Latent consistency (exclude k=0)\n",
    "        loss_lat = mse(z_pred[:, 1:, :], z_true[:, 1:, :])\n",
    "\n",
    "        # Stability penalty: penalize rho(K) > 1\n",
    "        rho = spectral_radius(model.K)\n",
    "        loss_stab = torch.relu(rho - 1.0) ** 2\n",
    "\n",
    "        loss = W_RECON * loss_recon + W_PRED * loss_pred + W_LAT * loss_lat + W_STAB * loss_stab\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if rank == 0 and (epoch == 1 or epoch % 50 == 0):\n",
    "        avg = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "        with torch.no_grad():\n",
    "            rho_val = float(spectral_radius(model.K).cpu().item())\n",
    "        print(f\"Epoch {epoch:5d}/{EPOCHS} | loss={avg:.6e} | rho(K)={rho_val:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# One-step forecast: predict t_next = last timestep + 1\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "t_next = times[-1] + 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_last = X_torch[-1].unsqueeze(0)             # (1,1,ny,nx) normalized\n",
    "    z_last = model.encode(x_last)                 # (1,z_dim)\n",
    "    z_next = model.step_latent(z_last)            # (1,z_dim)\n",
    "    x_next_n = model.decode(z_next, out_hw=(ny, nx))[0, 0].cpu().numpy()  # (ny,nx), normalized\n",
    "\n",
    "# Undo normalization back to physical values\n",
    "x_next_grid = (x_next_n * std_img[0] + mean_img[0]).astype(np.float64)   # (ny,nx)\n",
    "\n",
    "# Convert grid back to per-point ordering (coords_ref)\n",
    "if structured_ok:\n",
    "    x_next_pred = grid_to_points_structured(x_next_grid, ix=ix, iy=iy)   # (n_points,)\n",
    "else:\n",
    "    # fallback (not structured): sample via triangulation grid mapping is ambiguous\n",
    "    # We use nearest on the xs/ys grid indices as a pragmatic fallback.\n",
    "    # (If you end up here often, you should regrid your data to a structured mesh upstream.)\n",
    "    xi = np.searchsorted(xs, coords_ref[:, 0].astype(np.float64))\n",
    "    yi = np.searchsorted(ys, coords_ref[:, 1].astype(np.float64))\n",
    "    xi = np.clip(xi, 0, nx - 1)\n",
    "    yi = np.clip(yi, 0, ny - 1)\n",
    "    x_next_pred = x_next_grid[yi, xi].astype(np.float64)\n",
    "\n",
    "# Save prediction to CSV\n",
    "out_dir = field_path(Case(base_dir=BASE_DIR, phi=PHI, lat_size=LAT_SIZE, time_step=0, post=POST)).parent\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out = pd.DataFrame(coords_ref, columns=SORT_COLS)\n",
    "out[f\"{VAR_NAME}_pred\"] = x_next_pred\n",
    "out_path = out_dir / f\"koopmancnn_pred_{VAR_NAME}_{t_next}_xgt{int(X_THESHOLD)}.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Wrote:\", out_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compare to true next timestep if present + plots + XY error maps\n",
    "# ============================================================\n",
    "\n",
    "path_true = field_csv_path(BASE_DIR, PHI, LAT_SIZE, t_next, POST)\n",
    "\n",
    "# Build triangulation once for plotting on scattered coords\n",
    "x_sc = coords_ref[:, 0].astype(float)\n",
    "y_sc = coords_ref[:, 1].astype(float)\n",
    "tri_sc = mtri.Triangulation(x_sc, y_sc)\n",
    "try:\n",
    "    analyzer = mtri.TriAnalyzer(tri_sc)\n",
    "    mask = analyzer.get_flat_tri_mask(min_circle_ratio=0.02)\n",
    "    tri_sc.set_mask(mask)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def tricontour_field(vals_points: np.ndarray, title: str, cbar_label: str, vmin=None, vmax=None) -> None:\n",
    "    fig = plt.figure(figsize=(7.2, 5.8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cf = ax.tricontourf(tri_sc, vals_points, levels=60, vmin=vmin, vmax=vmax)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
    "    ax.set_title(title)\n",
    "    cbar = fig.colorbar(cf, ax=ax, orientation=\"horizontal\", pad=0.08, fraction=0.06)\n",
    "    cbar.set_label(cbar_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if path_true.exists():\n",
    "    coords_true_full, snap_true_full = read_field_sorted(path_true, VAR_NAME, SORT_COLS)\n",
    "\n",
    "    # coordinate match check to reference FULL coords\n",
    "    if coords_true_full.shape[0] != coords_ref_full.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"True next-step has different full point count: {coords_true_full.shape[0]} vs {coords_ref_full.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    if COORD_TOL == 0.0:\n",
    "        same = np.array_equal(coords_true_full, coords_ref_full)\n",
    "    else:\n",
    "        same = np.allclose(coords_true_full, coords_ref_full, atol=COORD_TOL, rtol=0.0)\n",
    "    if not same:\n",
    "        raise ValueError(\"True next-step coordinates do not match reference coordinates.\")\n",
    "\n",
    "    snap_true = snap_true_full[mask_x].astype(np.float64)\n",
    "\n",
    "    err = x_next_pred - snap_true\n",
    "    abs_err = np.abs(err)\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    rel_l2 = float(np.linalg.norm(err) / (np.linalg.norm(snap_true) + 1e-12))\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"Next-step compare at t={t_next}: RMSE={rmse:.6e}, relL2={rel_l2:.6e}\")\n",
    "\n",
    "    # Save error CSV (cropped)\n",
    "    out_err = pd.DataFrame(coords_ref, columns=SORT_COLS)\n",
    "    out_err[f\"{VAR_NAME}_true\"] = snap_true\n",
    "    out_err[f\"{VAR_NAME}_pred\"] = x_next_pred\n",
    "    out_err[\"err\"] = err\n",
    "    out_err[\"abs_err\"] = abs_err\n",
    "    err_path = out_dir / f\"koopmancnn_err_{VAR_NAME}_{t_next}_xgt{int(X_THESHOLD)}.csv\"\n",
    "    out_err.to_csv(err_path, index=False)\n",
    "    if rank == 0:\n",
    "        print(\"Wrote:\", err_path)\n",
    "\n",
    "    # Shared color scale for pred/true fields\n",
    "    vmin = float(min(np.percentile(snap_true, 0.5), np.percentile(x_next_pred, 0.5)))\n",
    "    vmax = float(max(np.percentile(snap_true, 99.5), np.percentile(x_next_pred, 99.5)))\n",
    "\n",
    "    tricontour_field(\n",
    "        x_next_pred,\n",
    "        title=f\"{VAR_NAME} PRED (Koopman-CNN) at t={t_next} (x > {X_THESHOLD})\",\n",
    "        cbar_label=f\"{VAR_NAME} (pred)\",\n",
    "        vmin=vmin, vmax=vmax,\n",
    "    )\n",
    "    tricontour_field(\n",
    "        snap_true,\n",
    "        title=f\"{VAR_NAME} TRUE at t={t_next} (x > {X_THESHOLD})\",\n",
    "        cbar_label=f\"{VAR_NAME} (true)\",\n",
    "        vmin=vmin, vmax=vmax,\n",
    "    )\n",
    "\n",
    "    # Side-by-side pred vs true (shared scale)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13.5, 5.6), dpi=110)\n",
    "    cf0 = axes[0].tricontourf(tri_sc, x_next_pred, levels=60, vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "    axes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_title(f\"{VAR_NAME} PRED (t={t_next})\")\n",
    "\n",
    "    cf1 = axes[1].tricontourf(tri_sc, snap_true, levels=60, vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "    axes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"y\")\n",
    "    axes[1].set_title(f\"{VAR_NAME} TRUE (t={t_next})\")\n",
    "\n",
    "    cbar = fig.colorbar(cf1, ax=axes.ravel().tolist(), orientation=\"horizontal\", pad=0.10, fraction=0.06)\n",
    "    cbar.set_label(f\"{VAR_NAME} (shared scale)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # True vs pred scatter\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(snap_true, x_next_pred, s=2)\n",
    "    lo = float(min(snap_true.min(), x_next_pred.min()))\n",
    "    hi = float(max(snap_true.max(), x_next_pred.max()))\n",
    "    plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted (Koopman-CNN)\")\n",
    "    plt.title(f\"{VAR_NAME}: true vs predicted at t={t_next} (x > {X_THESHOLD})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Error maps\n",
    "    vmax_signed = float(np.percentile(np.abs(err), 99.0)) + 1e-30\n",
    "    vmax_abs = float(np.percentile(abs_err, 99.0)) + 1e-30\n",
    "\n",
    "    tricontour_field(\n",
    "        err,\n",
    "        title=f\"{VAR_NAME} signed error (pred - true) at t={t_next} (x > {X_THESHOLD})\",\n",
    "        cbar_label=\"Signed error\",\n",
    "        vmin=-vmax_signed, vmax=vmax_signed,\n",
    "    )\n",
    "    tricontour_field(\n",
    "        abs_err,\n",
    "        title=f\"{VAR_NAME} absolute error |pred - true| at t={t_next} (x > {X_THESHOLD})\",\n",
    "        cbar_label=\"Absolute error\",\n",
    "        vmin=0.0, vmax=vmax_abs,\n",
    "    )\n",
    "\n",
    "    # Worst-points overlay\n",
    "    k = 300\n",
    "    idx_worst = np.argsort(abs_err)[-k:]\n",
    "    fig = plt.figure(figsize=(7.2, 5.8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_sc, y_sc, s=1)\n",
    "    ax.scatter(x_sc[idx_worst], y_sc[idx_worst], s=8)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(f\"Top-{k} worst points by |error| at t={t_next} (x > {X_THESHOLD})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    if rank == 0:\n",
    "        print(\"True next-step file does not exist; only the forecast was saved.\")\n",
    "\n",
    "    # Still plot predicted field\n",
    "    tricontour_field(\n",
    "        x_next_pred,\n",
    "        title=f\"{VAR_NAME} PRED (Koopman-CNN) at t={t_next} (x > {X_THESHOLD})\",\n",
    "        cbar_label=f\"{VAR_NAME} (pred)\",\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference timestep: 200\n",
      "X_THESHOLD=300 -> keeping 255744/839680 points\n",
      "n_points=255744, n_snapshots=70\n",
      "Reading: ..\\isocontours\\phi0.40\\h400x025_ref\\extracted_field_post_200.csv\n",
      "Grid inference: structured_ok=False, nx=875, ny=225, n_points=255744\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c66af73726effe43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}