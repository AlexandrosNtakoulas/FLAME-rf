{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16a3acec52643630"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T09:15:16.169488Z",
     "start_time": "2025-12-19T09:14:04.172290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# STRICT stability analysis:\n",
    "# Only reads EXACT (TIME_STEPS x ISOLEVELS) files you set.\n",
    "# Ranks candidate third feature z by kNN local variance of Sd\n",
    "# in [stretch, curvature, z]-space within each group (timestep).\n",
    "# Then aggregates ranks for stability.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from flamekit.io_fronts import Case, folder, front_path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "BASE_DIR   = Path(\"../data/isocontours\")\n",
    "PHI        = 0.40\n",
    "LAT_SIZE   = \"100\"\n",
    "POST       = True\n",
    "\n",
    "# Grouping axis for stability:\n",
    "#   \"timestep\"  -> ranks per timestep (recommended)\n",
    "#   \"isolevel\"  -> ranks per isolevel\n",
    "#   \"both\"      -> ranks per (timestep, isolevel)\n",
    "STABILITY_AXIS = \"timestep\"\n",
    "\n",
    "# Strict file selection (ONLY these will be read)\n",
    "ISOLEVELS   = [3,4.0,4.5]\n",
    "TIME_STEPS  = [140,150,160,170,180, 190, 200, 210]\n",
    "\n",
    "TARGET_COL  = \"DW_FDS\"          # Sd-like target\n",
    "STRETCH_COL = \"stretch_rate\"\n",
    "CURV_COL    = \"curvature\"\n",
    "\n",
    "KNN_K = 50\n",
    "MIN_ROWS_PER_GROUP = 5000\n",
    "SUBSAMPLE_PER_GROUP = None      # e.g. 20000 if you need speed\n",
    "\n",
    "TOPK_LIST = [10]\n",
    "\n",
    "SCALARS = [\n",
    "    \"H2\",\"O2\",\"H2O\",\"H\",\"O\",\"OH\",\"HO2\",\"H2O2\",\"HRR\",\n",
    "    \"omega_H2O\",\"omega_OH\",\n",
    "    \"curvature\",\"stretch_rate\",\"DW_FDS\",\n",
    "    \"abs_flame_prop_vel_normal\",\n",
    "    \"flow_velocity_normal\",\"flow_velocity_tangential\",\n",
    "    \"strain_rate\",\n",
    "    \"density_ratio_sigma\",\n",
    "    \"gradT\",\n",
    "    \"total_heat_conduction\",\n",
    "    \"H2_diffusion_total\",\"O2_diffusion_total\",\"H_diffusion_total\",\n",
    "    \"vorticity\",\n",
    "]\n",
    "\n",
    "# Exclude any z containing these substrings (target is excluded separately)\n",
    "EXCLUDE_SUBSTRINGS = [\"FDS\"]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# STRICT LOADER: only reads requested files\n",
    "# -------------------------\n",
    "def load_isocontour_data_strict(\n",
    "    base_dir: Path,\n",
    "    phi: float,\n",
    "    lat_size: str,\n",
    "    post: bool,\n",
    "    isolevels: list[float],\n",
    "    timesteps: list[int],\n",
    "    scalars: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    assert isolevels is not None and timesteps is not None, \"Set ISOLEVELS and TIME_STEPS (not None).\"\n",
    "\n",
    "    base_case = Case(base_dir=base_dir, phi=phi, lat_size=lat_size, time_step=0, post=post)\n",
    "    phi_dir = folder(base_case)\n",
    "    if not phi_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {phi_dir.resolve()}\")\n",
    "\n",
    "    files_to_read = []\n",
    "    for t in timesteps:\n",
    "        for iso in isolevels:\n",
    "            case = Case(base_dir=base_dir, phi=phi, lat_size=lat_size, time_step=t, post=post)\n",
    "            fp = front_path(case, float(iso))\n",
    "            found = fp if fp.exists() else None\n",
    "\n",
    "            if found is None:\n",
    "                tag = \"post_\" if post else \"\"\n",
    "                hits = sorted(phi_dir.glob(f\"extracted_flame_front_{tag}{t}_iso_{iso}*.csv\"))\n",
    "                if hits:\n",
    "                    found = hits[0]\n",
    "\n",
    "            if found is None:\n",
    "                raise FileNotFoundError(f\"Missing file for timestep={t}, iso={iso} in {phi_dir}\")\n",
    "\n",
    "            files_to_read.append(found)\n",
    "\n",
    "    print(\"Will read these files (STRICT):\")\n",
    "    for f in files_to_read:\n",
    "        print(\"  \", f.name)\n",
    "\n",
    "    frames = []\n",
    "    for fp in files_to_read:\n",
    "        df = pd.read_csv(fp)\n",
    "        df[\"__timestep__\"] = int(fp.stem.split(\"_post_\")[1].split(\"_iso_\")[0])\n",
    "        df[\"__isolevel__\"] = float(fp.stem.split(\"_iso_\")[1])\n",
    "        df[\"__source_file__\"] = fp.name\n",
    "        frames.append(df)\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Keep only requested scalar columns that exist + group tags\n",
    "    keep = [c for c in scalars if c in out.columns] + [\"__timestep__\", \"__isolevel__\", \"__source_file__\"]\n",
    "    return out[keep]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Metric: kNN local variance of Sd in 3D space\n",
    "# -------------------------\n",
    "def knn_nonuniqueness(d: pd.DataFrame, s_col: str, kappa_col: str, z_col: str, y_col: str, k: int):\n",
    "    X = d[[s_col, kappa_col, z_col]].to_numpy(float)\n",
    "    y = d[y_col].to_numpy(float)\n",
    "\n",
    "    if len(d) <= k + 2:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    nn = NearestNeighbors(n_neighbors=k + 1).fit(Xs)\n",
    "    idx = nn.kneighbors(Xs, return_distance=False)[:, 1:]  # drop self\n",
    "    local_var = np.var(y[idx], axis=1, ddof=1)\n",
    "\n",
    "    return float(np.mean(local_var)), float(np.median(local_var))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Rank z candidates within one group\n",
    "# -------------------------\n",
    "def rank_candidates_in_group(dfg: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [STRETCH_COL, CURV_COL, TARGET_COL]:\n",
    "        if c not in dfg.columns:\n",
    "            raise KeyError(f\"Missing required column: {c}\")\n",
    "\n",
    "    if len(dfg) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if SUBSAMPLE_PER_GROUP is not None and len(dfg) > SUBSAMPLE_PER_GROUP:\n",
    "        dfg = dfg.sample(SUBSAMPLE_PER_GROUP, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    exclude_exact = {STRETCH_COL, CURV_COL, TARGET_COL, \"__timestep__\", \"__isolevel__\", \"__source_file__\"}\n",
    "\n",
    "    candidates = []\n",
    "    for c in dfg.columns:\n",
    "        if c in exclude_exact:\n",
    "            continue\n",
    "        if any(sub in c for sub in EXCLUDE_SUBSTRINGS):\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(dfg[c]):\n",
    "            candidates.append(c)\n",
    "\n",
    "    base_cols = [STRETCH_COL, CURV_COL, TARGET_COL]\n",
    "    rows = []\n",
    "    for z in candidates:\n",
    "        d = dfg.dropna(subset=base_cols + [z])\n",
    "        if len(d) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "            continue\n",
    "        mean_var, med_var = knn_nonuniqueness(d, STRETCH_COL, CURV_COL, z, TARGET_COL, k=KNN_K)\n",
    "        if np.isfinite(mean_var):\n",
    "            rows.append({\"z_feature\": z, \"n\": len(d), \"mean_knn_var\": mean_var, \"median_knn_var\": med_var})\n",
    "\n",
    "    r = pd.DataFrame(rows)\n",
    "    if r.empty:\n",
    "        return r\n",
    "\n",
    "    r = r.sort_values([\"mean_knn_var\", \"median_knn_var\"], ascending=[True, True]).reset_index(drop=True)\n",
    "    r[\"rank\"] = np.arange(1, len(r) + 1)\n",
    "    return r\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Stability aggregation\n",
    "# -------------------------\n",
    "def stability_summary(per_group: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    g = per_group.copy()\n",
    "\n",
    "    summary = g.groupby(\"z_feature\").agg(\n",
    "        n_groups=(\"rank\", \"count\"),\n",
    "        mean_rank=(\"rank\", \"mean\"),\n",
    "        median_rank=(\"rank\", \"median\"),\n",
    "        std_rank=(\"rank\", \"std\"),\n",
    "        mean_mean_knn_var=(\"mean_knn_var\", \"mean\"),\n",
    "        median_mean_knn_var=(\"mean_knn_var\", \"median\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    nG = g[group_col].nunique()\n",
    "    for K in TOPK_LIST:\n",
    "        topk = g[g[\"rank\"] <= K].groupby(\"z_feature\")[group_col].nunique()\n",
    "        summary[f\"frac_top{K}\"] = summary[\"z_feature\"].map((topk / nG).to_dict()).fillna(0.0)\n",
    "\n",
    "    K0 = TOPK_LIST[0]\n",
    "    summary = summary.sort_values(\n",
    "        [\"mean_rank\", f\"frac_top{K0}\", \"mean_mean_knn_var\"],\n",
    "        ascending=[True, False, True]\n",
    "    ).reset_index(drop=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "df = load_isocontour_data_strict(BASE_DIR, PHI, LAT_SIZE, POST, ISOLEVELS, TIME_STEPS, SCALARS)\n",
    "\n",
    "if STABILITY_AXIS == \"timestep\":\n",
    "    group_keys = [\"__timestep__\"]\n",
    "elif STABILITY_AXIS == \"isolevel\":\n",
    "    group_keys = [\"__isolevel__\"]\n",
    "elif STABILITY_AXIS == \"both\":\n",
    "    group_keys = [\"__timestep__\", \"__isolevel__\"]\n",
    "else:\n",
    "    raise ValueError(\"STABILITY_AXIS must be one of: 'timestep', 'isolevel', 'both'\")\n",
    "\n",
    "per_group_tables = []\n",
    "for key, dfg in df.groupby(group_keys):\n",
    "    ranks = rank_candidates_in_group(dfg)\n",
    "    if ranks.empty:\n",
    "        continue\n",
    "\n",
    "    if isinstance(key, tuple):\n",
    "        for kname, kval in zip(group_keys, key):\n",
    "            ranks[kname] = kval\n",
    "    else:\n",
    "        ranks[group_keys[0]] = key\n",
    "\n",
    "    per_group_tables.append(ranks)\n",
    "\n",
    "if not per_group_tables:\n",
    "    raise RuntimeError(\"No groups produced rankings (too few rows per group, or too many NaNs).\")\n",
    "\n",
    "per_group = pd.concat(per_group_tables, ignore_index=True)\n",
    "\n",
    "# group id string\n",
    "if len(group_keys) == 1:\n",
    "    per_group[\"group_id\"] = per_group[group_keys[0]].astype(str)\n",
    "else:\n",
    "    per_group[\"group_id\"] = per_group[group_keys].astype(str).agg(\"_\".join, axis=1)\n",
    "\n",
    "per_group.to_csv(\"stability_per_group_rankings.csv\", index=False)\n",
    "print(\"Saved: stability_per_group_rankings.csv\")\n",
    "\n",
    "summary = stability_summary(per_group, group_col=\"group_id\")\n",
    "summary.to_csv(\"stability_summary.csv\", index=False)\n",
    "print(\"Saved: stability_summary.csv\")\n",
    "\n",
    "print(\"\\nTop 20 most stable z-features:\")\n",
    "print(summary.head(20).to_string(index=False))\n"
   ],
   "id": "9050d59691a0217f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read these files (STRICT):\n",
      "   extracted_flame_front_post_140_iso_3.0.csv\n",
      "   extracted_flame_front_post_140_iso_4.0.csv\n",
      "   extracted_flame_front_post_140_iso_4.5.csv\n",
      "   extracted_flame_front_post_150_iso_3.0.csv\n",
      "   extracted_flame_front_post_150_iso_4.0.csv\n",
      "   extracted_flame_front_post_150_iso_4.5.csv\n",
      "   extracted_flame_front_post_160_iso_3.0.csv\n",
      "   extracted_flame_front_post_160_iso_4.0.csv\n",
      "   extracted_flame_front_post_160_iso_4.5.csv\n",
      "   extracted_flame_front_post_170_iso_3.0.csv\n",
      "   extracted_flame_front_post_170_iso_4.0.csv\n",
      "   extracted_flame_front_post_170_iso_4.5.csv\n",
      "   extracted_flame_front_post_180_iso_3.0.csv\n",
      "   extracted_flame_front_post_180_iso_4.0.csv\n",
      "   extracted_flame_front_post_180_iso_4.5.csv\n",
      "   extracted_flame_front_post_190_iso_3.0.csv\n",
      "   extracted_flame_front_post_190_iso_4.0.csv\n",
      "   extracted_flame_front_post_190_iso_4.5.csv\n",
      "   extracted_flame_front_post_200_iso_3.0.csv\n",
      "   extracted_flame_front_post_200_iso_4.0.csv\n",
      "   extracted_flame_front_post_200_iso_4.5.csv\n",
      "   extracted_flame_front_post_210_iso_3.0.csv\n",
      "   extracted_flame_front_post_210_iso_4.0.csv\n",
      "   extracted_flame_front_post_210_iso_4.5.csv\n",
      "Saved: stability_per_group_rankings.csv\n",
      "Saved: stability_summary.csv\n",
      "\n",
      "Top 20 most stable z-features:\n",
      "                z_feature  n_groups  mean_rank  median_rank  std_rank  mean_mean_knn_var  median_mean_knn_var  frac_top10\n",
      "                      H2O         8      1.375          1.0  1.060660           0.015988             0.014419       1.000\n",
      "       O2_diffusion_total         8      3.250          2.0  2.052873           0.018875             0.016070       1.000\n",
      "                      HRR         8      3.875          4.0  1.457738           0.019017             0.016690       1.000\n",
      "                       O2         8      4.375          4.0  2.386719           0.019075             0.017209       1.000\n",
      "                       H2         8      5.000          4.0  2.563480           0.019659             0.017687       1.000\n",
      "    total_heat_conduction         8      5.875          6.0  2.295181           0.019842             0.016811       1.000\n",
      "                      HO2         8      5.875          6.5  1.726888           0.019853             0.017650       1.000\n",
      "                omega_H2O         8      7.625          7.5  1.302470           0.020558             0.018176       1.000\n",
      "                 omega_OH         8      8.375          8.0  1.302470           0.020824             0.018741       1.000\n",
      "                    gradT         8     10.125         10.0  1.552648           0.022337             0.020829       0.625\n",
      "                     H2O2         8     11.375         11.5  0.744024           0.023615             0.021589       0.125\n",
      "      density_ratio_sigma         8     11.875         12.5  1.726888           0.023782             0.021804       0.250\n",
      "                        H         8     12.250         12.5  1.164965           0.024026             0.022448       0.000\n",
      "       H2_diffusion_total         8     13.875         14.0  0.640870           0.025797             0.023687       0.000\n",
      "                        O         8     15.000         15.0  0.534522           0.028460             0.025520       0.000\n",
      "                       OH         8     16.125         16.0  0.640870           0.031560             0.029748       0.000\n",
      "        H_diffusion_total         8     16.750         17.0  0.462910           0.033747             0.031390       0.000\n",
      " flow_velocity_tangential         8     18.000         18.0  0.000000           0.040120             0.037587       0.000\n",
      "     flow_velocity_normal         8     19.000         19.0  0.000000           0.056861             0.051301       0.000\n",
      "abs_flame_prop_vel_normal         8     20.250         20.0  0.462910           0.069218             0.064149       0.000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T11:18:02.190535Z",
     "start_time": "2025-12-19T11:17:54.596672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 3-FEATURE SELECTION + STABILITY (NO assumed fixed features)\n",
    "#\n",
    "# Goal: find (f1, f2, f3) such that Sd (TARGET_COL) is as close\n",
    "# as possible to single-valued in [f1,f2,f3] space.\n",
    "#\n",
    "# Score(triple) = mean_kNN_var( Sd | neighbors in 3D feature space )\n",
    "#\n",
    "# Strict loader: reads ONLY the (TIME_STEPS x ISOLEVELS) files.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from flamekit.io_fronts import Case, folder, front_path\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "BASE_DIR   = Path(\"../data/isocontours\")\n",
    "PHI        = 0.40\n",
    "LAT_SIZE   = \"100\"\n",
    "POST       = True\n",
    "\n",
    "# Stability grouping:\n",
    "#   \"timestep\" | \"isolevel\" | \"both\"\n",
    "STABILITY_AXIS = \"isolevel\"\n",
    "\n",
    "# Strict file selection (ONLY these will be read)\n",
    "ISOLEVELS   = [3.0,3.5,3.8,4.0,4.5]\n",
    "TIME_STEPS  = [211]\n",
    "TARGET_COL  = \"DW_FDS\"   # Sd-like target\n",
    "\n",
    "# kNN parameters\n",
    "KNN_K = 50\n",
    "\n",
    "# Data thresholds / speed controls\n",
    "MIN_ROWS_PER_GROUP     = 0\n",
    "SUBSAMPLE_PER_GROUP    = None    # e.g. 20000 if too slow; None uses all\n",
    "\n",
    "# Candidate handling\n",
    "EXCLUDE_SUBSTRINGS = [\"FDS\"]      # excludes any feature containing these substrings\n",
    "M_PREFILTER        = 15           # reduce candidate set per group before triple search\n",
    "RANK_TOP_TRIPLES   = 30           # store top-N triples per group (for stability stats)\n",
    "TOPK_LIST          = [1, 3, 5, 10] # stability metric: frac groups where triple is in top-K\n",
    "\n",
    "SCALARS = [\n",
    "    \"curvature\",\n",
    "    \"DW_FDS\",\n",
    "    \"flow_velocity_normal\",\n",
    "    \"flow_velocity_tangential\",\n",
    "    \"strain_rate\",\n",
    "    \"tangential_strain_rate\",\n",
    "    \"normal_strain_rate\",\n",
    "    \"vorticity\",\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# STRICT LOADER (reads only requested combos)\n",
    "# ============================================================\n",
    "def load_isocontour_data_strict(base_dir: Path, phi: float, lat_size: str,\n",
    "    post: bool,\n",
    "                                isolevels: list[float], timesteps: list[int],\n",
    "                                scalars: list[str]) -> pd.DataFrame:\n",
    "    assert isolevels is not None and timesteps is not None, \"Set ISOLEVELS and TIME_STEPS (not None).\"\n",
    "\n",
    "    base_case = Case(base_dir=base_dir, phi=phi, lat_size=lat_size, time_step=0, post=post)\n",
    "    phi_dir = folder(base_case)\n",
    "    if not phi_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {phi_dir.resolve()}\")\n",
    "\n",
    "    files_to_read = []\n",
    "    for t in timesteps:\n",
    "        for iso in isolevels:\n",
    "            # Try common formatting variants\n",
    "            candidates = [\n",
    "                phi_dir / f\"extracted_flame_front_post_{t}_iso_{iso}.csv\",\n",
    "                phi_dir / f\"extracted_flame_front_post_{t}_iso_{iso:.1f}.csv\",\n",
    "                phi_dir / f\"extracted_flame_front_post_{t}_iso_{iso:.2f}.csv\",\n",
    "                phi_dir / f\"extracted_flame_front_post_{t}_iso_{iso:.3f}.csv\",\n",
    "            ]\n",
    "            found = next((p for p in candidates if p.exists()), None)\n",
    "            if found is None:\n",
    "                hits = sorted(phi_dir.glob(f\"extracted_flame_front_post_{t}_iso_{iso}*.csv\"))\n",
    "                if hits:\n",
    "                    found = hits[0]\n",
    "            if found is None:\n",
    "                raise FileNotFoundError(f\"Missing file for timestep={t}, iso={iso} in {phi_dir}\")\n",
    "            files_to_read.append(found)\n",
    "\n",
    "    print(\"Will read these files (STRICT):\")\n",
    "    for f in files_to_read:\n",
    "        print(\"  \", f.name)\n",
    "\n",
    "    frames = []\n",
    "    for fp in files_to_read:\n",
    "        df = pd.read_csv(fp)\n",
    "        # parse timestep & iso from stem\n",
    "        stem = fp.stem  # extracted_flame_front_post_210_iso_4.5\n",
    "        t = int(stem.split(\"_post_\")[1].split(\"_iso_\")[0])\n",
    "        iso = float(stem.split(\"_iso_\")[1])\n",
    "        df[\"__timestep__\"] = t\n",
    "        df[\"__isolevel__\"] = iso\n",
    "        df[\"__source_file__\"] = fp.name\n",
    "        frames.append(df)\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    keep = [c for c in scalars if c in out.columns] + [\"__timestep__\", \"__isolevel__\", \"__source_file__\"]\n",
    "    return out[keep]\n",
    "\n",
    "# ============================================================\n",
    "# kNN local-variance score (dimension-agnostic)\n",
    "# ============================================================\n",
    "def knn_local_variance_score(d: pd.DataFrame, feat_cols: list[str], y_col: str, k: int):\n",
    "    \"\"\"\n",
    "    Score = mean over points of Var(y among kNN in feature space)\n",
    "    Lower => closer to single-valued mapping y = f(features).\n",
    "    \"\"\"\n",
    "    X = d[feat_cols].to_numpy(float)\n",
    "    y = d[y_col].to_numpy(float)\n",
    "\n",
    "    if len(d) <= k + 2:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k + 1).fit(Xs)\n",
    "    idx = nn.kneighbors(Xs, return_distance=False)[:, 1:]  # drop self\n",
    "    local_var = np.var(y[idx], axis=1, ddof=1)\n",
    "\n",
    "    return float(np.mean(local_var)), float(np.median(local_var))\n",
    "\n",
    "# ============================================================\n",
    "# Candidate extraction\n",
    "# ============================================================\n",
    "def get_candidates(df: pd.DataFrame) -> list[str]:\n",
    "    exclude_exact = {TARGET_COL, \"__timestep__\", \"__isolevel__\", \"__source_file__\"}\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude_exact:\n",
    "            continue\n",
    "        if any(sub in c for sub in EXCLUDE_SUBSTRINGS):\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            candidates.append(c)\n",
    "    return candidates\n",
    "\n",
    "# ============================================================\n",
    "# Rank triples in one group (prefilter then full triple search)\n",
    "# ============================================================\n",
    "def rank_triples_in_group(dfg: pd.DataFrame) -> pd.DataFrame:\n",
    "    if TARGET_COL not in dfg.columns:\n",
    "        raise KeyError(f\"Missing required column: {TARGET_COL}\")\n",
    "\n",
    "    # basic group size / subsample\n",
    "    if len(dfg) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if SUBSAMPLE_PER_GROUP is not None and len(dfg) > SUBSAMPLE_PER_GROUP:\n",
    "        dfg = dfg.sample(SUBSAMPLE_PER_GROUP, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    candidates = get_candidates(dfg)\n",
    "    if len(candidates) < 3:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Drop rows missing target early\n",
    "    dfg = dfg.dropna(subset=[TARGET_COL])\n",
    "    if len(dfg) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ---- Stage 1: Prefilter using 1D kNN variance (fast)\n",
    "    oneD_rows = []\n",
    "    for f in candidates:\n",
    "        d = dfg.dropna(subset=[f, TARGET_COL])\n",
    "        if len(d) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "            continue\n",
    "        mean_var, med_var = knn_local_variance_score(d, [f], TARGET_COL, KNN_K)\n",
    "        if np.isfinite(mean_var):\n",
    "            oneD_rows.append((f, mean_var, med_var, len(d)))\n",
    "\n",
    "    if not oneD_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    oneD = pd.DataFrame(oneD_rows, columns=[\"feature\", \"mean_var_1d\", \"median_var_1d\", \"n\"])\n",
    "    oneD = oneD.sort_values([\"mean_var_1d\", \"median_var_1d\"], ascending=[True, True])\n",
    "\n",
    "    pre = oneD[\"feature\"].head(M_PREFILTER).tolist()\n",
    "    if len(pre) < 3:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ---- Stage 2: Full triple search over prefiltered set\n",
    "    rows = []\n",
    "    base_needed = [TARGET_COL] + pre\n",
    "    dbase = dfg.dropna(subset=[TARGET_COL])  # keep target non-NaN\n",
    "\n",
    "    for f1, f2, f3 in combinations(pre, 3):\n",
    "        d = dbase.dropna(subset=[f1, f2, f3, TARGET_COL])\n",
    "        if len(d) < max(MIN_ROWS_PER_GROUP, KNN_K + 5):\n",
    "            continue\n",
    "        mean_var, med_var = knn_local_variance_score(d, [f1, f2, f3], TARGET_COL, KNN_K)\n",
    "        if np.isfinite(mean_var):\n",
    "            rows.append({\n",
    "                \"f1\": f1, \"f2\": f2, \"f3\": f3,\n",
    "                \"n\": len(d),\n",
    "                \"mean_knn_var\": mean_var,\n",
    "                \"median_knn_var\": med_var\n",
    "            })\n",
    "\n",
    "    r = pd.DataFrame(rows)\n",
    "    if r.empty:\n",
    "        return r\n",
    "\n",
    "    r = r.sort_values([\"mean_knn_var\", \"median_knn_var\"], ascending=[True, True]).reset_index(drop=True)\n",
    "    r[\"rank\"] = np.arange(1, len(r) + 1)\n",
    "    return r\n",
    "\n",
    "# ============================================================\n",
    "# Stability aggregation over groups\n",
    "# ============================================================\n",
    "def stability_summary_triples(per_group: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    g = per_group.copy()\n",
    "    g[\"triple\"] = g.apply(lambda r: tuple(sorted([r[\"f1\"], r[\"f2\"], r[\"f3\"]])), axis=1)\n",
    "\n",
    "    summary = g.groupby(\"triple\").agg(\n",
    "        n_groups=(\"rank\", \"count\"),\n",
    "        mean_rank=(\"rank\", \"mean\"),\n",
    "        median_rank=(\"rank\", \"median\"),\n",
    "        std_rank=(\"rank\", \"std\"),\n",
    "        mean_mean_knn_var=(\"mean_knn_var\", \"mean\"),\n",
    "        median_mean_knn_var=(\"mean_knn_var\", \"median\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    nG = g[group_col].nunique()\n",
    "    for K in TOPK_LIST:\n",
    "        topk = g[g[\"rank\"] <= K].groupby(\"triple\")[group_col].nunique()\n",
    "        summary[f\"frac_top{K}\"] = summary[\"triple\"].map((topk / nG).to_dict()).fillna(0.0)\n",
    "\n",
    "    # Sort: stable best = low mean rank, high frac_top1, then low mean var\n",
    "    summary = summary.sort_values(\n",
    "        [\"mean_rank\", \"frac_top1\", \"mean_mean_knn_var\"],\n",
    "        ascending=[True, False, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Expand triple into columns for readability\n",
    "    summary[[\"t1\", \"t2\", \"t3\"]] = pd.DataFrame(summary[\"triple\"].tolist(), index=summary.index)\n",
    "    return summary\n",
    "\n",
    "def feature_frequency(best_triple_per_group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Counts how often each feature appears in the BEST triple (rank=1) across groups\n",
    "    feats = []\n",
    "    for _, r in best_triple_per_group.iterrows():\n",
    "        feats.extend([r[\"f1\"], r[\"f2\"], r[\"f3\"]])\n",
    "    s = pd.Series(feats).value_counts().rename_axis(\"feature\").reset_index(name=\"count_in_best_triples\")\n",
    "    s[\"frac_groups\"] = s[\"count_in_best_triples\"] / best_triple_per_group[\"group_id\"].nunique()\n",
    "    return s\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "df = load_isocontour_data_strict(BASE_DIR, PHI, LAT_SIZE, POST, ISOLEVELS, TIME_STEPS, SCALARS)\n",
    "\n",
    "if STABILITY_AXIS == \"timestep\":\n",
    "    group_keys = [\"__timestep__\"]\n",
    "elif STABILITY_AXIS == \"isolevel\":\n",
    "    group_keys = [\"__isolevel__\"]\n",
    "elif STABILITY_AXIS == \"both\":\n",
    "    group_keys = [\"__timestep__\", \"__isolevel__\"]\n",
    "else:\n",
    "    raise ValueError(\"STABILITY_AXIS must be one of: 'timestep', 'isolevel', 'both'\")\n",
    "\n",
    "per_group_tables = []\n",
    "for key, dfg in df.groupby(group_keys):\n",
    "    ranked = rank_triples_in_group(dfg)\n",
    "    if ranked.empty:\n",
    "        continue\n",
    "\n",
    "    # attach group identifiers\n",
    "    if isinstance(key, tuple):\n",
    "        for kname, kval in zip(group_keys, key):\n",
    "            ranked[kname] = kval\n",
    "        group_id = \"_\".join(str(v) for v in key)\n",
    "    else:\n",
    "        ranked[group_keys[0]] = key\n",
    "        group_id = str(key)\n",
    "\n",
    "    ranked[\"group_id\"] = group_id\n",
    "\n",
    "    # keep only top-N triples per group for stability statistics\n",
    "    ranked = ranked.head(RANK_TOP_TRIPLES)\n",
    "    per_group_tables.append(ranked)\n",
    "\n",
    "if not per_group_tables:\n",
    "    raise RuntimeError(\"No groups produced rankings. Reduce MIN_ROWS_PER_GROUP or check NaNs / filters.\")\n",
    "\n",
    "per_group = pd.concat(per_group_tables, ignore_index=True)\n",
    "\n",
    "# Save per-group triple rankings\n",
    "per_group.to_csv(\"stability_per_group_triple_rankings.csv\", index=False)\n",
    "print(\"Saved: stability_per_group_triple_rankings.csv\")\n",
    "\n",
    "# Best triple per group\n",
    "best_per_group = per_group.sort_values([\"group_id\", \"rank\"]).groupby(\"group_id\", as_index=False).first()\n",
    "best_per_group.to_csv(\"stability_best_triple_per_group.csv\", index=False)\n",
    "print(\"Saved: stability_best_triple_per_group.csv\")\n",
    "\n",
    "# Stability summary over triples\n",
    "summary = stability_summary_triples(per_group, group_col=\"group_id\")\n",
    "summary.to_csv(\"stability_triple_summary.csv\", index=False)\n",
    "print(\"Saved: stability_triple_summary.csv\")\n",
    "\n",
    "# Feature frequency in best triples\n",
    "freq = feature_frequency(best_per_group)\n",
    "freq.to_csv(\"stability_feature_frequency_in_best_triples.csv\", index=False)\n",
    "print(\"Saved: stability_feature_frequency_in_best_triples.csv\")\n",
    "\n",
    "print(\"\\nBest triple per group:\")\n",
    "print(best_per_group[[\"group_id\",\"f1\",\"f2\",\"f3\",\"mean_knn_var\",\"median_knn_var\",\"n\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 20 most stable triples:\")\n",
    "print(summary.head(20)[[\"t1\",\"t2\",\"t3\",\"n_groups\",\"mean_rank\",\"frac_top1\",\"frac_top3\",\"frac_top5\",\"mean_mean_knn_var\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nMost frequent features in BEST triples:\")\n",
    "print(freq.head(20).to_string(index=False))\n"
   ],
   "id": "45362fb642a47af6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read these files (STRICT):\n",
      "   extracted_flame_front_post_211_iso_3.0.csv\n",
      "   extracted_flame_front_post_211_iso_3.5.csv\n",
      "   extracted_flame_front_post_211_iso_3.8.csv\n",
      "   extracted_flame_front_post_211_iso_4.0.csv\n",
      "   extracted_flame_front_post_211_iso_4.5.csv\n",
      "Saved: stability_per_group_triple_rankings.csv\n",
      "Saved: stability_best_triple_per_group.csv\n",
      "Saved: stability_triple_summary.csv\n",
      "Saved: stability_feature_frequency_in_best_triples.csv\n",
      "\n",
      "Best triple per group:\n",
      "group_id                       f1                   f2                   f3  mean_knn_var  median_knn_var    n\n",
      "     3.0                curvature          strain_rate flow_velocity_normal      0.021552        0.003118 3331\n",
      "     3.5                curvature          strain_rate flow_velocity_normal      0.026315        0.003946 3553\n",
      "     3.8                curvature flow_velocity_normal          strain_rate      0.041639        0.005204 3781\n",
      "     4.0     flow_velocity_normal            curvature          strain_rate      0.041520        0.005592 3963\n",
      "     4.5 flow_velocity_tangential flow_velocity_normal          strain_rate      0.047347        0.007360 8824\n",
      "\n",
      "Top 20 most stable triples:\n",
      "                      t1                       t2                       t3  n_groups  mean_rank  frac_top1  frac_top3  frac_top5  mean_mean_knn_var\n",
      "               curvature     flow_velocity_normal              strain_rate         5   2.000000        0.8        0.8        0.8           0.038473\n",
      "               curvature              strain_rate   tangential_strain_rate         4   4.750000        0.0        0.0        0.6           0.045934\n",
      "               curvature       normal_strain_rate              strain_rate         4   5.250000        0.0        0.2        0.6           0.046052\n",
      "    flow_velocity_normal       normal_strain_rate              strain_rate         5   5.600000        0.0        0.6        0.6           0.042024\n",
      "    flow_velocity_normal              strain_rate   tangential_strain_rate         5   6.400000        0.0        0.6        0.6           0.042266\n",
      "               curvature     flow_velocity_normal flow_velocity_tangential         5   8.400000        0.0        0.0        0.0           0.051369\n",
      "               curvature     flow_velocity_normal                vorticity         5   9.600000        0.0        0.0        0.2           0.053972\n",
      "               curvature     flow_velocity_normal   tangential_strain_rate         5  12.000000        0.0        0.2        0.2           0.056948\n",
      "    flow_velocity_normal flow_velocity_tangential       normal_strain_rate         5  12.400000        0.0        0.0        0.2           0.052721\n",
      "               curvature flow_velocity_tangential              strain_rate         5  14.200000        0.0        0.0        0.0           0.056178\n",
      "    flow_velocity_normal flow_velocity_tangential                vorticity         3  14.333333        0.0        0.2        0.2           0.067127\n",
      "    flow_velocity_normal flow_velocity_tangential              strain_rate         4  14.500000        0.2        0.2        0.4           0.058733\n",
      "               curvature flow_velocity_tangential                vorticity         5  14.600000        0.0        0.0        0.0           0.055732\n",
      "    flow_velocity_normal flow_velocity_tangential   tangential_strain_rate         5  15.000000        0.0        0.2        0.2           0.053465\n",
      "flow_velocity_tangential   tangential_strain_rate                vorticity         5  15.000000        0.0        0.0        0.4           0.053485\n",
      "flow_velocity_tangential              strain_rate   tangential_strain_rate         5  15.600000        0.0        0.0        0.0           0.055519\n",
      "               curvature     flow_velocity_normal       normal_strain_rate         5  15.800000        0.0        0.0        0.0           0.061328\n",
      "flow_velocity_tangential       normal_strain_rate              strain_rate         5  16.200000        0.0        0.0        0.0           0.055627\n",
      "               curvature              strain_rate                vorticity         3  16.333333        0.0        0.0        0.0           0.047021\n",
      "flow_velocity_tangential       normal_strain_rate                vorticity         5  16.400000        0.0        0.0        0.0           0.054152\n",
      "\n",
      "Most frequent features in BEST triples:\n",
      "                 feature  count_in_best_triples  frac_groups\n",
      "             strain_rate                      5          1.0\n",
      "    flow_velocity_normal                      5          1.0\n",
      "               curvature                      4          0.8\n",
      "flow_velocity_tangential                      1          0.2\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6cb51e25a70aed95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}