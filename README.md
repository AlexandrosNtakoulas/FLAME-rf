
### MACHINE-LEARNING AND MODEL REDUCTION FRAMEWORK DEVELOPMENT FOR DNS DATA
Python-based open source frameworks
offer powerful platforms to assist the
construction of reduced order models
as well as for machine learning applications.
The aim of this project is to develop a
general framework that will simplify
and accelerate the application of such
tools on high-fidelity data generated by
spectral element solvers for the direct
numerical simulation (DNS) of low
Mach number reactive flows. The
framework will be used on existing
DNS data from different setups. Depending
on progress, new simulations
will also be performed for lean hydrogen-
air premixed flames to enrich the
existing datasets.

This repository contains the codebase developed as part of my Bachelor Thesis at **ETH ZÃ¼rich (D-MAVT)**, conducted in the **Computational and Applied Physics / Combustion Laboratory (CAPS)**.  
The project focuses on **data-driven modeling of hydrogen combustion** using **Direct Numerical Simulation (DNS)** data obtained from the **Nek5000** spectral-element solver.

---

## ðŸ“˜ Overview

The goal of this work is to establish a reproducible computational pipeline that:
1. Extracts local flame characteristics (e.g., curvature, strain, species concentrations) from high-fidelity DNS data.
2. Performs **feature scaling, dimensionality reduction, and regression** to uncover physical relations governing the **flame displacement speed**.
3. Bridges **physics-based modeling** with **data-driven discovery** through interpretable and efficient machine-learning models.

---

## ðŸ§© Project Structure

````text.
â”œâ”€â”€ data/                         # Nek5000 output data (NOT tracked by git)
â”‚   â”œâ”€â”€ phi0.40/
â”‚   â”‚   â”œâ”€â”€ h400x025_ref/
â”‚   â”‚   â”‚   â”œâ”€â”€ po_postPremix0.f00001
â”‚   â”‚   â”‚   â”œâ”€â”€ po_postPremix0.f00100
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ h400x050_ref/
â”‚   â”‚   â”œâ”€â”€ h400x100_ref/
â”‚   â”‚   â””â”€â”€ h400x800_ref/
â”‚   â””â”€â”€ phi0.50/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ isocontours/                  # Extracted flame-front CSVs (NOT tracked by git)
â”‚   â”œâ”€â”€ phi0.40/
â”‚   â”‚   â”œâ”€â”€ h400x200_ref/
â”‚   â”‚   â”‚   â”œâ”€â”€ extracted_flame_front_post_<TIME>_iso_<ISO>.csv
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/                    # Analysis notebooks (clustering, PCA, ML, plots, etc.)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ flamekit/                     
â”‚   â”œâ”€â”€ datasets.py               # SEMDataset wrapper (pysemtools/Nek interfacing)
â”‚   â”œâ”€â”€ io_fronts.py              # Case + load_fronts() CSV I/O utilities
â”‚   â””â”€â”€ ML_models - old/          # Legacy models (kept for reference)
â”‚
â”œâ”€â”€ pySEMTools/                   # Local pysemtools source/clone (core SEM functionality)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ test/                         # Tests / experiments
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
````
## Usage

### 1) Set up the environment
1. Create and activate a Python environment (recommended).
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
### 2) Place Nek5000 output files in the correct directory (not tracked by git)
````text
data/
â””â”€â”€ phi0.40/
    â””â”€â”€ h400x025_ref/
        â”œâ”€â”€ po_postPremix0.f00001   # REQUIRED: always include the first time step
        â”œâ”€â”€ po_postPremix0.f00335   # example time step you want to analyze
        â””â”€â”€ ...

````
#### Notes:
- The folder structure encodes the case: phi{PHI}/h400x{LAT_SIZE}_ref
- The file prefix depends on whether you use post-processing fields:
  - POST=True, for po_postPremix0.fXXXXX 
  - POST=False, for premix0.fXXXXX
- Always include the first time step file (...f00001) in the same folder, even if you analyze a later time step.
### 3) Extract flame fronts (create CSVs)
1. Open the flame-front extraction notebook in notebooks/.
2. Set the key parameters: PHI, LAT_SIZE, TIME_STEP, POST , ISOLEVELS (the list of isotherm / progress-variable levels)
3. output directory: ISOCONTOUR_BASE_DIR = Path("../isocontours")
4. Run the notebook.

````text
isocontours/
â””â”€â”€ phi0.40/
    â””â”€â”€ h400x025_ref/
        â”œâ”€â”€ extracted_flame_front_post_<TIME_STEP>_iso_<ISO>.csv
        â””â”€â”€ ...

````
### 4) Run analysis notebooks (read the CSVs)
All downstream notebooks should read the extracted CSVs via the unified I/O:
````bash
from flamekit.io_fronts import Case, load_fronts
from pathlib import Path

BASE_DIR   = Path("../isocontours")
PHI        = 0.40
LAT_SIZE   = "025"
TIME_STEP  = 335
POST       = True
ISOLEVELS  = [4.5, 4.6, 4.7]

case = Case(
    base_dir=BASE_DIR,
    phi=PHI,
    lat_size=LAT_SIZE,
    time_step=TIME_STEP,
    post=POST,
)

fronts = load_fronts(case, ISOLEVELS)   # dict[iso] -> DataFrame
df_45  = fronts[4.5]                   # use isotherm 4.5

````
From here you can proceed with:
* clustering (KMeans/GMM),
* PCA per cluster,
* per-cluster Sd prediction + permutation feature importance,
* interactive Plotly visualizations, etc.